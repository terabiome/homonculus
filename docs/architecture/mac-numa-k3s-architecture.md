# Mac Mini + NUMA Server K3s Architecture

## Overview

This document describes the **hybrid K3s cluster** architecture with a Mac Mini M2 as the pure control plane and a dual-socket NUMA server running 3 worker VMs: 2 for SLM compute and 1 for storage/data processing.

---

## System Topology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mac Mini M2 (Pure Control Plane)                  â”‚
â”‚  â”œâ”€ K3s Master (control plane only)                 â”‚
â”‚  â”œâ”€ VictoriaLogs (log aggregation + UI)             â”‚
â”‚  â”œâ”€ VictoriaMetrics (metrics + UI)                  â”‚
â”‚  â”œâ”€ Temporal Server (workflow orchestration)        â”‚
â”‚  â””â”€ Temporal UI                                      â”‚
â”‚                                                      â”‚
â”‚  Hardware:                                           â”‚
â”‚  â”œâ”€ CPU: M2 (8 cores)                               â”‚
â”‚  â”œâ”€ RAM: 8GB                                         â”‚
â”‚  â”œâ”€ Storage: VM disk only (~20GB)                   â”‚
â”‚  â””â”€ Network: 1GbE (192.168.1.10)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ K3s Cluster Network
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NUMA Server (Compute + Storage)                    â”‚
â”‚  â”œâ”€ VM1: SLM Worker 1 (NUMA Node 0)                 â”‚
â”‚  â”‚   â”œâ”€ 17 cores (34 threads)                       â”‚
â”‚  â”‚   â”œâ”€ Emulator: 1 core (2 threads) reserved       â”‚
â”‚  â”‚   â”œâ”€ SLM inference pods (3 models)               â”‚
â”‚  â”‚   â””â”€ Temporal workers (co-located)               â”‚
â”‚  â”‚                                                   â”‚
â”‚  â”œâ”€ VM2: SLM Worker 2 (NUMA Node 1)                 â”‚
â”‚  â”‚   â”œâ”€ 17 cores (34 threads)                       â”‚
â”‚  â”‚   â”œâ”€ Emulator: 1 core (2 threads) reserved       â”‚
â”‚  â”‚   â”œâ”€ SLM inference pods (3 models)               â”‚
â”‚  â”‚   â””â”€ Temporal workers (co-located)               â”‚
â”‚  â”‚                                                   â”‚
â”‚  â””â”€ VM3: MinIO Storage + Data Processing            â”‚
â”‚      â”œâ”€ 6 cores (flexible placement)                â”‚
â”‚      â”œâ”€ MinIO (8TB "Locness Lake" data gateway)     â”‚
â”‚      â”œâ”€ Data processing pods (DuckDB/Polars)        â”‚
â”‚      â””â”€ Temporal workers (prep/aggregation)         â”‚
â”‚                                                      â”‚
â”‚  Hardware:                                           â”‚
â”‚  â”œâ”€ CPU: 2Ã— E5-2697v4 (36 cores, 72 threads)        â”‚
â”‚  â”œâ”€ RAM: 252GB                                       â”‚
â”‚  â”œâ”€ SSD: ~350GB (VM disks + staging)                â”‚
â”‚  â”œâ”€ HDD: 8TB (MinIO data lake on VM3)               â”‚
â”‚  â””â”€ Network: 1GbE (192.168.1.20)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Design Rationale

### **Why Mac Mini as Master?**

```yaml
Strengths:
âœ… M2 is fast (overkill for control plane)
âœ… Low power (~15-20W vs 200W for dual-socket)
âœ… Already purchased (make it useful!)
âœ… Perfect for lightweight services (monitoring, orchestration)
âœ… Can run headless (SSH only, saves RAM)

Constraints:
âš ï¸ Only 8GB RAM (tight but manageable)
âš ï¸ macOS quirks (sleep, updates, overhead)
âš ï¸ Single point of failure (acceptable for home lab)
âš ï¸ K3s requires Linux VM (Multipass/Lima)

Solution:
â”œâ”€ Run only control plane + lightweight monitoring
â”œâ”€ NO compute workloads on Mac
â”œâ”€ NO heavy storage (MinIO on NUMA server instead)
â”œâ”€ Aggressive resource limits on pods
â””â”€ Let NUMA server handle all compute + storage
```

### **Why 3 VMs (Not 2)?**

```yaml
Original plan: 2 SLM VMs only
New plan: 2 SLM VMs + 1 Storage VM

Reason:
â”œâ”€ Mac can't efficiently handle 8TB HDD (virtualization overhead)
â”œâ”€ MinIO benefits from native Linux filesystem (ext4)
â”œâ”€ Data processing should be near storage (minimize network I/O)
â”œâ”€ Clean separation: Compute (VM1/VM2) vs Storage (VM3)
â””â”€ ALL 34 threads per VM for SLM (maximum compute)

VM3 Role:
â”œâ”€ MinIO server (8TB HDD, native Linux)
â”œâ”€ Data processing pods (DuckDB/Polars)
â”œâ”€ Temporal workers (daily prep, aggregation)
â””â”€ Uses leftover CPU threads (emulator + extras)

Benefits:
âœ… Native disk performance (125 MB/s HDD, no virtualization)
âœ… Clean architecture (control, compute, storage separated)
âœ… VM1/VM2 stay pure compute (34 vCPUs each for SLM)
âœ… Network overhead acceptable (5% of job time)
âœ… Simpler than distributed MinIO across VM1/VM2
```

### **Why Temporal Workers on SLM VMs?**

```yaml
Key insight: Temporal workers need local filesystem access!

SLM workflow:
1. Temporal worker fetches input from MinIO
2. Worker writes to local staging disk (/data/local)
3. SLM pod reads from local staging disk
4. SLM pod writes results to local staging disk
5. Temporal worker uploads results to MinIO

Benefits of co-location:
âœ… No network overhead (local filesystem)
âœ… Fast I/O (SSD on worker VM)
âœ… Simpler data flow (no NFS/CSI)
âœ… Temporal worker sees SLM output immediately
â””â”€ Perfect for "write-local, sweep-remote" pattern! ğŸ¯
```

---

## VM Specifications

### **VM1: k3s-slm-worker-1 (NUMA Node 0)**

```json
{
  "name": "k3s-slm-worker-1",
  "vcpu_count": 34,
  "memory_mb": 122880,
  "disk_path": "/var/lib/libvirt/images/slm1.qcow2",
  "disk_size_gb": 35,
  "base_image_path": "/var/lib/libvirt/images/ubuntu-22.04-base.qcow2",
  "bridge_network_interface": "br0",
  "tuning": {
    "vcpu_pins": [
      "0","36","1","37","2","38","3","39",
      "4","40","5","41","6","42","7","43",
      "8","44","9","45","10","46","11","47",
      "12","48","13","49","14","50","15","51",
      "16","52"
    ],
    "emulator_cpuset": "17,53",
    "numa_memory": {
      "nodeset": "0",
      "mode": "strict"
    }
  }
}
```

**Resources:**
- Cores: 17 physical (34 threads) - 1 core reserved for emulator
- Memory: 120GB
- NUMA: Node 0 (strict locality)
- Storage: 35GB OS + 100GB staging disk
- Emulator: Thread 17, 53 (dedicated)

**Workloads:**
```yaml
SLM Pods (3 models):
â”œâ”€ Gemma-7B (9B): 6-10 cores, ~30GB RAM
â”œâ”€ Mistral-7B (7.3B): 6-10 cores, ~30GB RAM
â””â”€ Gemma-4B (4B): 6-10 cores, ~20GB RAM

Temporal Workers (DaemonSet):
â”œâ”€ Workflow orchestration: 2 cores (guaranteed), ~4-8GB RAM
â”œâ”€ Daily data prep (DuckDB/Polars processing)
â”œâ”€ Pre-job data fetch + staging
â”œâ”€ Post-job result sweep + upload
â””â”€ Pattern aggregation + distribution

Total per VM:
â”œâ”€ Guaranteed: 20 cores (2 + 18), 84GB RAM
â”œâ”€ Burst capacity: 32 cores, 110GB RAM
â”œâ”€ Headroom: 2 cores, 10GB RAM âœ…
â””â”€ Fits comfortably in 34 cores, 120GB! ğŸ¯
```

---

### **VM2: k3s-slm-worker-2 (NUMA Node 1)**

```json
{
  "name": "k3s-slm-worker-2",
  "vcpu_count": 34,
  "memory_mb": 122880,
  "disk_path": "/var/lib/libvirt/images/slm2.qcow2",
  "disk_size_gb": 35,
  "base_image_path": "/var/lib/libvirt/images/ubuntu-22.04-base.qcow2",
  "bridge_network_interface": "br0",
  "tuning": {
    "vcpu_pins": [
      "18","54","19","55","20","56","21","57",
      "22","58","23","59","24","60","25","61",
      "26","62","27","63","28","64","29","65",
      "30","66","31","67","32","68","33","69",
      "34","70"
    ],
    "emulator_cpuset": "35,71",
    "numa_memory": {
      "nodeset": "1",
      "mode": "strict"
    }
  }
}
```

**Resources:**
- Cores: 17 physical (34 threads) - 1 core reserved for emulator
- Memory: 120GB
- NUMA: Node 1 (strict locality)
- Storage: 35GB OS + 100GB staging disk
- Emulator: Thread 35, 71 (dedicated)

**Workloads:**
```yaml
SLM Pods (3 models):
â”œâ”€ Qwen-7B (7B): 6-10 cores, ~25GB RAM
â”œâ”€ Gemma-2B (2.5B): 6-10 cores, ~12GB RAM
â””â”€ Phi-3-Mini (3.8B): 6-10 cores, ~18GB RAM

Temporal Workers (DaemonSet):
â”œâ”€ Workflow orchestration: 2 cores (guaranteed), ~4-8GB RAM
â”œâ”€ Daily data prep (DuckDB/Polars processing)
â”œâ”€ Pre-job data fetch + staging
â”œâ”€ Post-job result sweep + upload
â””â”€ Pattern aggregation + distribution

Total per VM:
â”œâ”€ Guaranteed: 20 cores (2 + 18), 63GB RAM
â”œâ”€ Burst capacity: 32 cores, 95GB RAM
â”œâ”€ Headroom: 2 cores, 25GB RAM âœ…
â””â”€ Perfect symmetry with VM1! ğŸ¯
```

---

### **VM3: k3s-minio-storage (Flexible Placement)**

```json
{
  "name": "k3s-minio-storage",
  "vcpu_count": 6,
  "memory_mb": 32768,
  "disk_path": "/var/lib/libvirt/images/minio.qcow2",
  "disk_size_gb": 35,
  "base_image_path": "/var/lib/libvirt/images/ubuntu-22.04-base.qcow2",
  "bridge_network_interface": "br0",
  "tuning": {
    "vcpu_pins": [
      "17","53","35","71","18","54"
    ],
    "emulator_cpuset": null,
    "numa_memory": null
  }
}
```

**Resources:**
- Cores: 6 vCPUs (flexible, uses emulator threads + extras)
- Memory: 32GB
- NUMA: No strict affinity (can span nodes)
- Storage: 35GB OS disk + 8TB HDD (passthrough)
- Emulator: Shared with host (no dedicated pinning)

**Workloads:**
```yaml
MinIO (Single-node):
â”œâ”€ S3-compatible API: 2 cores, ~500MB-1GB RAM
â”œâ”€ 8TB HDD (native ext4)
â””â”€ Network I/O bound (not CPU bound)

Data Processing Pods:
â”œâ”€ DuckDB/Polars: 2-4 cores, ~10-20GB RAM
â”œâ”€ Daily prep, aggregation tasks
â””â”€ Temporal workers (co-located with data)

Temporal Workers (DaemonSet):
â”œâ”€ Daily data prep (reads/writes MinIO locally)
â”œâ”€ Aggregation (reads all results from MinIO)
â””â”€ Pattern merging (local operations)

Total per VM:
â”œâ”€ MinIO: 2 cores, 1GB
â”œâ”€ Data processing: 2-4 cores, 10-20GB
â”œâ”€ Temporal: 1 core, 4GB
â”œâ”€ Total: ~6 cores, 15-25GB
â””â”€ Headroom: 7-17GB âœ…
```

**HDD Passthrough:**
```bash
# 8TB HDD attached as /dev/sdb on host
# Passed through to VM3 as /dev/vdb
# Formatted as ext4 and mounted at /mnt/datalake
```

**Why flexible CPU placement?**
- MinIO is I/O bound (disk + network), not CPU intensive
- Data processing is batch (not latency sensitive)
- Can use leftover CPU cycles from either NUMA node
- No strict NUMA affinity needed

---

## Mac Mini Resource Allocation

### **Memory Budget (8GB Total)**

```yaml
macOS (base system): ~3GB
â”œâ”€ Kernel: ~1GB
â”œâ”€ WindowServer: ~300MB (or 0 if headless)
â”œâ”€ Background services: ~500MB
â””â”€ File cache: ~1.2GB

Lima/Multipass VM (4GB):
â”œâ”€ Ubuntu guest OS: ~800MB
â”œâ”€ K3s control plane: ~800MB
â”‚   â”œâ”€ API server: ~300MB
â”‚   â”œâ”€ Scheduler: ~100MB
â”‚   â”œâ”€ Controller manager: ~150MB
â”‚   â”œâ”€ Etcd: ~200MB
â”‚   â””â”€ kubelet: ~50MB
â”œâ”€ Pods on Master:
â”‚   â”œâ”€ VictoriaLogs: ~150-200MB
â”‚   â”œâ”€ VictoriaMetrics: ~150-200MB
â”‚   â”œâ”€ Temporal Server: ~1-1.5GB
â”‚   â”œâ”€ Temporal UI: ~150-200MB
â”‚   â””â”€ Promtail: ~64-128MB
â””â”€ Total VM: ~3.5-4GB

Total Mac RAM Usage:
â”œâ”€ macOS: 3GB
â”œâ”€ VM: 4GB
â””â”€ Headroom: 1GB âœ…

Comfortable! Mac can run headless (disable WindowServer) for more RAM.
```

### **Storage Allocation (VM Only)**

```yaml
Lima/Multipass VM disk (20GB):
â”œâ”€ Ubuntu OS: ~5GB
â”œâ”€ K3s + containerd: ~3GB
â”œâ”€ Pod images: ~5GB
â”œâ”€ Victoria* data (7-day retention): ~2GB
â”œâ”€ Temporal DB (30-day history): ~2GB
â””â”€ Logs + temp: ~3GB

Total: ~20GB (fits in VM disk)
No external HDD needed on Mac! âœ…
```

---

## Data Flow

### **Phase 1: Daily Data Prep (Morning, Once Per Day)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Temporal Server (Mac)                            â”‚
â”‚     â””â”€ Triggers daily prep workflow (06:00 AM)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Temporal Worker (VM1 or VM2)                     â”‚
â”‚     â”œâ”€ Fetch from MinIO:                             â”‚
â”‚     â”‚  â”œâ”€ s3://lake/market-data/last-7-days/         â”‚
â”‚     â”‚  â””â”€ s3://lake/patterns/last-30-days/           â”‚
â”‚     â”œâ”€ Process with DuckDB/Polars (in worker pod)    â”‚
â”‚     â”œâ”€ Generate: /data/local/daily_context_<date>.pq â”‚
â”‚     â”œâ”€ Size: 2-5GB                                   â”‚
â”‚     â””â”€ Time: 5-10 minutes                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Upload to MinIO (Mac)                            â”‚
â”‚     â”œâ”€ Upload: s3://lake/daily-prepared/<date>.pq   â”‚
â”‚     â”œâ”€ Cleanup: Delete /data/local/daily_prep_*     â”‚
â”‚     â””â”€ Network: 16-40 seconds (1GbE)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: Daily context ready for all jobs today
Cached locally: /data/local/daily_context_<date>.parquet
```

### **Phase 2: Per-Job Workflow (Per Symbol, e.g., BTC)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Pre-Job Data Prep (Temporal Worker)           â”‚
â”‚  â”œâ”€ Fetch pre-swept data from MinIO                    â”‚
â”‚  â”œâ”€ MOVE s3://lake/pre-swept/<symbol>/ â†’               â”‚
â”‚  â”‚      s3://lake/daily-archived/<date>/               â”‚
â”‚  â”œâ”€ Write: /data/local/input_<job-id>_<symbol>.pq     â”‚
â”‚  â””â”€ Time: 1-2 seconds (atomic operation)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: SLM Inference (6-7 Models in Parallel)        â”‚
â”‚  â”œâ”€ Read inputs (concurrent, safe):                    â”‚
â”‚  â”‚  â”œâ”€ /data/local/input_<job-id>_<symbol>.pq         â”‚
â”‚  â”‚  â”œâ”€ /data/local/daily_context_<date>.pq (cached!)  â”‚
â”‚  â”‚  â””â”€ /data/local/patterns_<date>.pq (cached!)       â”‚
â”‚  â”œâ”€ SLM inference (3-4 minutes)                        â”‚
â”‚  â””â”€ Write outputs (unique files):                      â”‚
â”‚     â”œâ”€ /data/local/output_<model>_<job-id>.pq         â”‚
â”‚     â””â”€ /data/local/patterns_new_<model>_<job-id>.pq   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Result Sweep (Temporal Workers on VM1 + VM2)  â”‚
â”‚  â”œâ”€ Collect: /data/local/output_*_<job-id>.pq          â”‚
â”‚  â”œâ”€ Upload: s3://lake/staging/results/<job-id>/        â”‚
â”‚  â”œâ”€ Collect: /data/local/patterns_new_*_<job-id>.pq    â”‚
â”‚  â”œâ”€ Upload: s3://lake/staging/patterns/<job-id>/       â”‚
â”‚  â”œâ”€ Keep local: /data/local/patterns_<date>.pq         â”‚
â”‚  â”œâ”€ Cleanup: Delete intermediate files                  â”‚
â”‚  â””â”€ Time: 10-30 seconds per VM                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Aggregation (Temporal Worker, Either VM)      â”‚
â”‚  â”œâ”€ Download all results from staging (6-7 files)      â”‚
â”‚  â”œâ”€ Weighted consensus calculation                      â”‚
â”‚  â”œâ”€ Upload: s3://lake/gold/predictions/<symbol>_<date>â”‚
â”‚  â”œâ”€ Merge patterns: /data/local/patterns_merged.pq     â”‚
â”‚  â”œâ”€ Upload: s3://lake/patterns/<date>/unified.pq       â”‚
â”‚  â””â”€ Time: 30-60 seconds                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: Pattern Distribution (Both VMs)               â”‚
â”‚  â”œâ”€ Download: s3://lake/patterns/<date>/unified.pq     â”‚
â”‚  â”œâ”€ Cache locally: /data/local/patterns_<date>.pq      â”‚
â”‚  â””â”€ Ready for next job! (cache hit 100%)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total per job: ~5-6 minutes
Daily capacity: ~80-100 symbols (16 hours Ã— 6 min/job)
```

### **Key Pattern: Co-located Workers + Local Caching**

```yaml
Why co-location matters:

Traditional (BAD):
â”œâ”€ SLM pod writes to PV (network storage)
â”œâ”€ Temporal worker on different VM reads from PV
â””â”€ Need NFS/CSI + 2Ã— network overhead!

Co-located (GOOD):
â”œâ”€ SLM pod + Temporal worker on SAME VM
â”œâ”€ Both mount /data/local (hostPath, same directory)
â”œâ”€ SLM writes to /data/local (local SSD, 500 MB/s)
â”œâ”€ Temporal reads from /data/local (instant!)
â”œâ”€ Temporal uploads to MinIO (single network hop)
â””â”€ Zero network overhead for hot path! âœ…

Caching strategy:
â”œâ”€ Daily context (2-5GB): Fetched 1Ã— per day
â”œâ”€ Patterns (50-500MB): Fetched 1Ã— per day
â”œâ”€ Both cached in /data/local
â””â”€ Reused across ALL jobs same day (100% cache hit!)

Performance gain:
â”œâ”€ Local read/write: ~500 MB/s (SSD)
â”œâ”€ Network read/write: ~125 MB/s (1GbE)
â”œâ”€ Cache hit: No network at all!
â””â”€ 4Ã— faster + zero network for cached data! ğŸš€
```

### **File Lifecycle on /data/local**

```yaml
Long-lived (Cached Daily):
â”œâ”€ daily_context_<date>.parquet (2-5GB, 1 day)
â””â”€ patterns_<date>.parquet (50-500MB, 1 day)

Short-lived (Per Job):
â”œâ”€ input_<job-id>_<symbol>.parquet (50-100MB)
â”œâ”€ output_<model>_<job-id>.parquet (50-100MB Ã— 6-7)
â””â”€ patterns_new_<model>_<job-id>.parquet (10-50MB)
   â†“ Deleted after upload to MinIO

Disk usage per VM:
â”œâ”€ Cached: 3-5GB (daily + patterns)
â”œâ”€ Active job: 1-2GB (inputs + outputs)
â”œâ”€ Peak: 5-7GB
â””â”€ 100GB staging disk: 93-95GB free âœ…

Concurrency safety:
â”œâ”€ Read-only files: Multiple SLM pods read simultaneously âœ…
â”œâ”€ Write files: Each pod writes unique filename âœ…
â””â”€ No file locking needed! âœ…
```

---

## Kubernetes Configuration

### **Node Taints and Labels (Whitelist Strategy)**

```yaml
Mac Master:
â”œâ”€ Role: control-plane
â”œâ”€ Taint: node-role.kubernetes.io/control-plane:NoSchedule
â””â”€ Labels: role=master, storage=true

VM1 (SLM Worker 1):
â”œâ”€ Role: worker
â”œâ”€ Taints: NONE (whitelist approach!)
â””â”€ Labels: role=worker, workload=slm, numa-node=0

VM2 (SLM Worker 2):
â”œâ”€ Role: worker
â”œâ”€ Taints: NONE (whitelist approach!)
â””â”€ Labels: role=worker, workload=slm, numa-node=1

Why no taints?
â”œâ”€ Whitelist via nodeSelector (explicit opt-in)
â”œâ”€ SLM pods must request workload=slm
â”œâ”€ Temporal workers request role=worker
â”œâ”€ Flexible for future workload types
â””â”€ No need to manage tolerations! âœ…
```

### **Pod Scheduling Strategy**

```yaml
Master Node Pods (Mac):
  nodeSelector:
    role: master
  tolerations:
  - key: node-role.kubernetes.io/control-plane
    effect: NoSchedule
  
  Pods:
  â”œâ”€ MinIO (pinned to master)
  â”œâ”€ VictoriaLogs (pinned to master)
  â”œâ”€ VictoriaMetrics (pinned to master)
  â”œâ”€ Temporal Server (pinned to master)
  â””â”€ Temporal UI (pinned to master)

SLM Worker Pods (VM1, VM2):
  nodeSelector:
    workload: slm          # Must run on SLM nodes
  
  affinity:
    podAntiAffinity:       # Spread across NUMA nodes
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values: [slm-inference]
          topologyKey: numa-node
  
  volumes:
  - name: local-staging
    hostPath:
      path: /data/local    # Shared with Temporal worker!
      type: Directory

Temporal Workers (DaemonSet):
  nodeSelector:
    role: worker           # Runs on ALL worker nodes (VM1, VM2)
  
  volumes:
  - name: local-staging
    hostPath:
      path: /data/local    # Same as SLM pods!
      type: DirectoryOrCreate
  
  resources:              # CRITICAL: Prevent CPU starvation
    requests:
      cpu: "2000m"        # Guaranteed 2 cores
      memory: "4Gi"       # Guaranteed 4GB
    limits:
      cpu: "2000m"        # No burst (strict reservation)
      memory: "8Gi"       # Can use up to 8GB
```

### **Resource Guarantees (Prevent CPU Starvation)**

```yaml
Problem:
â”œâ”€ SLM pods can use all 34 cores during inference
â”œâ”€ Temporal worker needs CPU to sweep results
â””â”€ Without guarantees: Temporal gets starved! âš ï¸

Solution: K8s Resource Requests

Per VM (34 cores, 120GB RAM):

Temporal Worker (Guaranteed QoS):
  requests:
    cpu: "2000m"         # K8s reserves 2 cores
    memory: "4Gi"        # K8s reserves 4GB
  limits:
    cpu: "2000m"         # No burst (strict)
    memory: "8Gi"
  
  Priority: CRITICAL
  â””â”€ Always gets 2 cores, even during SLM inference âœ…

SLM Pods (Burstable QoS):
  requests:
    cpu: "6000m"         # Guaranteed 6 cores each
    memory: "20Gi"       # Guaranteed 20GB each
  limits:
    cpu: "10000m"        # Can burst to 10 cores
    memory: "30Gi"       # Can use up to 30GB
  
  Priority: HIGH
  â””â”€ Can burst when Temporal idle, throttled when needed

Example: 3 SLM models per VM:
â”œâ”€ Temporal: 2 cores (guaranteed)
â”œâ”€ SLM 1: 6-10 cores (burstable)
â”œâ”€ SLM 2: 6-10 cores (burstable)
â”œâ”€ SLM 3: 6-10 cores (burstable)
â”œâ”€ Total guaranteed: 2 + 18 = 20 cores
â”œâ”€ Total burst: 2 + 30 = 32 cores
â””â”€ Fits in 34 cores! âœ…

How K8s enforces:
â”œâ”€ Scheduling: Reserves guaranteed resources
â”œâ”€ Runtime: CPU quota enforcement (cgroups)
â”œâ”€ Pressure: Throttles burstable pods first
â””â”€ Result: Temporal never starved! ğŸ¯
```

---

## Network Services

### **Exposed Services (NodePort)**

```yaml
Mac Mini (192.168.1.10):
â”œâ”€ MinIO API: :30900
â”œâ”€ MinIO Console: :30901
â”œâ”€ VictoriaLogs UI: :30428
â”œâ”€ VictoriaMetrics UI: :30828
â”œâ”€ Temporal UI: :30880
â””â”€ K3s API: :6443

Access from anywhere:
â”œâ”€ MinIO: http://192.168.1.10:30901
â”œâ”€ Logs: http://192.168.1.10:30428/select/vmui
â”œâ”€ Metrics: http://192.168.1.10:30828/vmui
â””â”€ Temporal: http://192.168.1.10:30880
```

### **Internal Services (ClusterIP)**

```yaml
Within cluster:
â”œâ”€ minio.storage.svc.cluster.local:9000
â”œâ”€ victorialogs.monitoring.svc.cluster.local:9428
â”œâ”€ victoriametrics.monitoring.svc.cluster.local:8428
â”œâ”€ temporal.temporal.svc.cluster.local:7233
â””â”€ All accessible via DNS
```

---

## Performance Characteristics

### **SLM Inference**

```yaml
6-Model Ensemble (25k context):
â”œâ”€ Processing: 3-4 minutes (parallel)
â”œâ”€ Memory: ~140GB / 220GB (64%)
â”œâ”€ CPU: 56 threads utilized
â””â”€ Quality: Excellent (3 large + 3 small)

Throughput:
â”œâ”€ Sequential: ~15 jobs/hour
â”œâ”€ Parallel (2 symbols): ~24 jobs/hour
â””â”€ Daily capacity: ~360 jobs (16 hours)
```

### **Data Transfers**

```yaml
Daily prep (VM3 â†’ MinIO):
â”œâ”€ Size: 2-5GB
â”œâ”€ Time: 16-40 seconds (1GbE)
â””â”€ Frequency: Once per day

SLM input fetch (MinIO â†’ VM1/VM2):
â”œâ”€ Size: 50-100MB per worker
â”œâ”€ Time: 1-2 seconds
â””â”€ Frequency: Per job

SLM result upload (VM1/VM2 â†’ MinIO):
â”œâ”€ Size: 100-400MB per model
â”œâ”€ Time: 5-20 seconds
â””â”€ Frequency: Per job (6 models)

Total network per job: ~1-3GB
Time on network: ~10-30 seconds
Network utilization: ~1-2% of job time âœ…
```

---

## Failure Modes

### **Mac Mini Failure**

```yaml
Impact:
â”œâ”€ K3s control plane unavailable
â”œâ”€ No new pod scheduling
â”œâ”€ MinIO unavailable (data lake offline)
â”œâ”€ Temporal server unavailable
â””â”€ Workers keep running (existing pods OK)

Recovery:
â”œâ”€ Reboot Mac Mini
â”œâ”€ K3s auto-starts
â”œâ”€ Pods restart automatically
â””â”€ Time: ~2-5 minutes

Prevention:
â”œâ”€ Disable macOS sleep
â”œâ”€ Disable auto-updates
â”œâ”€ Monitor with external ping
```

### **Worker VM Failure**

```yaml
VM1 or VM2 fails:
â”œâ”€ Impact: 3 SLM models offline + Temporal worker offline
â”œâ”€ Cluster still has 3 models running on other VM
â”œâ”€ Jobs take 2Ã— longer (50% capacity loss)
â”œâ”€ Daily prep can still run on surviving VM
â””â”€ Pattern distribution only to surviving VM

Recovery:
â”œâ”€ Restart VM via libvirt (virsh start <vm>)
â”œâ”€ K3s agent reconnects automatically
â”œâ”€ Pods rescheduled (SLM inference resumes)
â”œâ”€ Temporal worker re-downloads cached files
â””â”€ Time: ~2-3 minutes

Mitigation:
â”œâ”€ Monitor VM health (systemd watchdog)
â”œâ”€ Automate VM restart on failure
â”œâ”€ Temporal workflows auto-retry on worker failure
â””â”€ MinIO staging acts as buffer (no data loss)

Both VMs fail:
â”œâ”€ Impact: NO SLM inference capacity
â”œâ”€ Mac continues to run (control plane OK)
â”œâ”€ Jobs queued until VMs recover
â””â”€ Manual intervention required
```

---

## Monitoring Strategy

### **VictoriaLogs Queries**

```
# All SLM pod logs
_stream:{namespace="slm"}

# Temporal worker logs
_stream:{pod=~"temporal-worker.*"}

# Errors across cluster
error OR failed OR exception

# Recent job completions
_stream:{namespace="slm"} | "job completed"
```

### **VictoriaMetrics Queries**

```
# Node CPU usage
100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Pod memory usage
container_memory_usage_bytes{namespace="slm"}

# SLM inference time
histogram_quantile(0.95, rate(slm_inference_duration_seconds_bucket[5m]))

# Temporal job success rate
rate(temporal_workflow_completed_total{status="success"}[5m])
```

---

## Cost Analysis

### **Power Consumption**

```yaml
Mac Mini M2:
â”œâ”€ Idle: 10-15W
â”œâ”€ Load: 25-35W
â””â”€ Average: ~20W

Dual E5-2697v4:
â”œâ”€ Idle: 150-200W
â”œâ”€ Load: 350-450W
â””â”€ Average: ~250W

Total: ~270W average
Monthly: ~195 kWh
Cost: ~$20-30/month (electricity)

vs Single NUMA server: $40-60/month
Savings: $20-30/month âœ…
```

### **Operational Costs**

```yaml
Monthly:
â”œâ”€ Electricity: ~$25
â”œâ”€ Internet: $0 (existing)
â”œâ”€ LLM API calls: $10-50 (variable)
â””â”€ Total: ~$35-75/month

Per job:
â”œâ”€ Compute: ~$0.01 (electricity)
â”œâ”€ LLM API: ~$0.05-0.10
â””â”€ Total: ~$0.06-0.11 per analysis

Very cost-effective for home lab! ğŸ’°
```

---

## Upgrade Path

### **When Mac Becomes Bottleneck**

```yaml
Option 1: Add RAM to Mac
â”œâ”€ Upgrade to 16GB or 24GB
â”œâ”€ Cost: N/A (M2 RAM is soldered ğŸ’€)
â””â”€ Verdict: Not possible! âŒ

Option 2: Replace Mac with Mini PC
â”œâ”€ Intel N100 / AMD 5700U
â”œâ”€ 16-32GB RAM
â”œâ”€ Cost: $200-400
â””â”€ Verdict: Good future option âœ…

Option 3: Move master to VM on NUMA server
â”œâ”€ Create 4th VM for K3s master
â”œâ”€ Use Mac as pure storage (just MinIO)
â”œâ”€ Cost: $0 (use existing hardware)
â””â”€ Verdict: Best option! â­
```

### **When Need More Compute**

```yaml
Option 1: Add more NUMA nodes
â”œâ”€ More E5-2697v4 or newer CPUs
â”œâ”€ More worker VMs
â””â”€ Scales linearly

Option 2: Add second NUMA server
â”œâ”€ Another dual-socket machine
â”œâ”€ Join to same K3s cluster
â””â”€ Double capacity

Option 3: Cloud burst
â”œâ”€ Use cloud for peak loads
â”œâ”€ Expensive but flexible
â””â”€ Hybrid approach
```

---

## References

- Original NUMA design: `docs/architecture/numa-design.md`
- Distributed architecture: `docs/architecture/distributed-architecture.md`
- VM specifications: `docs/architecture/vm-specifications.md`
- Finance use case: `docs/use-cases/finance-ensemble.md`
- Deployment manifests: `k8s/`

---

## Summary

**Architecture Type:** Hybrid K3s cluster (Mac Mini + NUMA server)

**Key Characteristics:**
- âœ… Cost-effective (repurpose Mac Mini failure ğŸ’€)
- âœ… Power-efficient (Mac for control plane)
- âœ… NUMA-aware (strict locality for SLM VMs)
- âœ… Co-located workers (Temporal + SLM on same node)
- âœ… Centralized storage (8TB "Locness Lake" ğŸ´ó §ó ¢ó ³ó £ó ´ó ¿)
- âœ… Built-in monitoring (Victoria stack with UIs)
- âœ… Production-ready (for home lab scale)

**Perfect for:** Finance analysis, SLM ensembles, data processing workloads requiring fast local I/O and centralized storage!

**Life lesson:** Even purchasing mistakes can be turned into useful infrastructure! ğŸ˜‚ğŸ’ª

