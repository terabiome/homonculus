# Distributed Architecture

## Overview

This document describes the **distributed two-machine architecture** - an evolution of the original single-machine NUMA design. This architecture separates compute-intensive workloads (SLM inference) from data-intensive workloads (data lake and processing).

---

## Architecture Evolution

### **Original Design: Single NUMA Machine**
- All workloads on one dual-socket machine
- 4 VMs: SLM workers + data node + task node
- Storage co-located with compute

### **Distributed Design: Two-Machine Cluster**
- **Machine 1 (NUMA)**: Pure compute workers for SLM inference
- **Machine 2 (UMA)**: K3s control plane + data lake + data processing
- Clear separation of concerns
- Connected via 1GbE network

---

## Machine Overview

### **Machine 1: NUMA Worker Cluster**

**Hardware:**
- Dual Intel Xeon E5-2696v4 (2Ã—18 cores, 36 cores total)
- 252GB RAM (2Ã—126GB)
- 350GB SSD
- 1GbE network

**Role:** Pure compute - SLM inference workloads

**VMs:**
- 2 large worker nodes (1 per NUMA socket)
- Each VM: 16 cores (32 threads), 118GB RAM

---

### **Machine 2: UMA Data Server**

**Hardware:**
- Intel Xeon E3-1270v6 (4 cores, single socket)
- 64GB RAM
- SSD (OS) + 8TB HDD (data lake)
- 1GbE network

**Role:** 
- K3s control plane (master)
- Data lake storage (8TB HDD)
- Data processing and orchestration
- Temporal workflow engine

---

## Network Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Machine 1: NUMA Worker Cluster                    â”‚
â”‚   (Dual E5-2696v4, 252GB RAM, 350GB SSD)           â”‚
â”‚                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚  VM1 (Worker 1)  â”‚  â”‚  VM2 (Worker 2)  â”‚       â”‚
â”‚   â”‚  Node 0          â”‚  â”‚  Node 1          â”‚       â”‚
â”‚   â”‚  16c/32t, 118GB  â”‚  â”‚  16c/32t, 118GB  â”‚       â”‚
â”‚   â”‚  35GB + 100GB    â”‚  â”‚  35GB + 100GB    â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚            â”‚                     â”‚                  â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                       â”‚ Bridge (br0)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ 1GbE Network
                        â”‚ (~125 MB/s)
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       â”‚                              â”‚
â”‚   Machine 2: UMA Data Server                        â”‚
â”‚   (E3-1270v6, 64GB RAM, 8TB HDD)                    â”‚
â”‚                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  K3s Control Plane                   â”‚          â”‚
â”‚   â”‚  - API Server                        â”‚          â”‚
â”‚   â”‚  - Scheduler                         â”‚          â”‚
â”‚   â”‚  - Controller Manager                â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  Temporal Orchestrator               â”‚          â”‚
â”‚   â”‚  - Workflow engine                   â”‚          â”‚
â”‚   â”‚  - Activity workers                  â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  Data Services                       â”‚          â”‚
â”‚   â”‚  - Daily data preparation            â”‚          â”‚
â”‚   â”‚  - Pattern memory server             â”‚          â”‚
â”‚   â”‚  - Result aggregation                â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                      â”‚
â”‚   ğŸ’¾ Data Lake (8TB HDD)                            â”‚
â”‚   â””â”€ /data/lake                                     â”‚
â”‚       â”œâ”€ clean/bronze/                              â”‚
â”‚       â”œâ”€ clean/silver/                              â”‚
â”‚       â”œâ”€ clean/gold/                                â”‚
â”‚       â”œâ”€ patterns/                                  â”‚
â”‚       â”œâ”€ archive/                                   â”‚
â”‚       â””â”€ raw/                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow Architecture

### **Phase 1: Start of Day - Data Preparation**

```
Data Server                    1GbE Network                Worker Cluster
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prepare Data â”‚                                          â”‚              â”‚
â”‚ - Query lake â”‚                                          â”‚              â”‚
â”‚ - Transform  â”‚                                          â”‚              â”‚
â”‚ - Compress   â”‚                                          â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚              â”‚
       â”‚                                                  â”‚              â”‚
       â”‚ Push daily inputs (1-5GB per worker)           â”‚              â”‚
       â”‚ + patterns (if shared, ~1GB)                    â”‚              â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚ Stage to     â”‚
       â”‚                                                  â”‚ /data/local/ â”‚
       â”‚ Transfer time: ~1-2 minutes @ 125MB/s          â”‚ (100GB SSD)  â”‚
       â”‚                                                  â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚              â”‚
                                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Phase 2: During the Day - Inference**

```
Worker Cluster (Local Operations Only)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚  1. Read input from /data/local/inputs     â”‚
â”‚     (Local SSD, ~500 MB/s)                 â”‚
â”‚            â†“                                â”‚
â”‚  2. [Optional] Fetch pattern memory        â”‚
â”‚     (From data server, ~0.8s for 100MB)    â”‚
â”‚            â†“                                â”‚
â”‚  3. Run SLM inference                      â”‚
â”‚     (5-10 minutes, CPU-intensive)          â”‚
â”‚            â†“                                â”‚
â”‚  4. Write results to /data/local/outputs   â”‚
â”‚     (Local SSD, ~500 MB/s)                 â”‚
â”‚            â†“                                â”‚
â”‚  5. Signal Temporal: "Job Complete"        â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Network Usage During Inference: ~0 bytes
(Except optional pattern fetch: 10-100MB)
```

### **Phase 3: Job Completion - Result Transfer**

```
Worker Cluster                1GbE Network                Data Server
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job Complete â”‚                                         â”‚ Temporal     â”‚
â”‚   Signal     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Receives     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚ Signal       â”‚
                                                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Results in   â”‚   Orchestrator pulls results            â”‚ Trigger      â”‚
â”‚ /data/local/ â”‚   (100MB-4GB total, all SLMs)          â”‚ Transfer     â”‚
â”‚ /outputs/    â”‚  <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ Activity     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
       Transfer time: 8-32 seconds @ 125MB/s                   â”‚
       (Typical: ~16 seconds for 2GB)                          â”‚
                                                         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                                                         â”‚ Write to     â”‚
                                                         â”‚ Lake         â”‚
                                                         â”‚ /data/lake/  â”‚
                                                         â”‚ clean/silver/â”‚
                                                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cleanup      â”‚   Signal: "Transfer Complete"           â”‚ Aggregate    â”‚
â”‚ Staging      â”‚  <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ Results      â”‚
â”‚ Delete files â”‚                                         â”‚ Write to goldâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Worker VM Specifications

### **VM1: k3s-worker-1 (NUMA Node 0)**

```json
{
  "name": "k3s-worker-1",
  "vcpu_count": 32,
  "memory_mb": 120832,
  "tuning": {
    "vcpu_pins": [
      "0", "36", "1", "37", "2", "38", "3", "39",
      "4", "40", "5", "41", "6", "42", "7", "43",
      "8", "44", "9", "45", "10", "46", "11", "47",
      "12", "48", "13", "49", "14", "50", "15", "51"
    ],
    "emulator_cpuset": "16-17,52-53",
    "numa_memory": {
      "nodeset": "0",
      "mode": "strict"
    }
  }
}
```

**Storage:**
- OS disk: 35GB (`/dev/vda`)
- Staging disk: 100GB (`/dev/vdb` â†’ `/data/local`)

**Storage Layout:**
```
/dev/vda (35GB): OS
â””â”€ /: System files, binaries, logs

/dev/vdb (100GB): Staging
â”œâ”€ /data/local/inputs: Daily input data (1-5GB)
â”‚   â””â”€ daily_inputs_2025-11-10.parquet
â”‚
â”œâ”€ /data/local/outputs: Job results (cleaned after transfer)
â”‚   â”œâ”€ slm_1_job_123.parquet (~100MB-2GB)
â”‚   â”œâ”€ slm_2_job_123.parquet
â”‚   â””â”€ ... (deleted after successful transfer)
â”‚
â””â”€ /data/local/cache: Pattern memory (optional)
    â””â”€ patterns_*.pkl (~10-100MB, LRU cache)

Typical usage: 5-15GB / 100GB = 5-15%
Peak usage: ~20GB / 100GB = 20%
Headroom: 80GB+ for spikes âœ…
```

---

### **VM2: k3s-worker-2 (NUMA Node 1)**

```json
{
  "name": "k3s-worker-2",
  "vcpu_count": 32,
  "memory_mb": 120832,
  "tuning": {
    "vcpu_pins": [
      "18", "54", "19", "55", "20", "56", "21", "57",
      "22", "58", "23", "59", "24", "60", "25", "61",
      "26", "62", "27", "63", "28", "64", "29", "65",
      "30", "66", "31", "67", "32", "68", "33", "69"
    ],
    "emulator_cpuset": "34-35,70-71",
    "numa_memory": {
      "nodeset": "1",
      "mode": "strict"
    }
  }
}
```

**Storage:** Same as VM1

---

## Data Server Configuration

### **K3s Control Plane**

```bash
# Install K3s in server mode
curl -sfL https://get.k3s.io | sh -s - server \
  --disable traefik \
  --node-name data-server

# Get join token for workers
sudo cat /var/lib/rancher/k3s/server/node-token
```

### **Storage Layout**

```
/dev/sda (SSD): OS + processing
â”œâ”€ /: System files
â”œâ”€ /var/lib/rancher/k3s: K3s data
â”œâ”€ /opt/temporal: Temporal server
â””â”€ /tmp: Temporary processing

/dev/sdb (8TB HDD): Data lake
â””â”€ /data/lake:
    â”œâ”€ clean/
    â”‚   â”œâ”€ bronze/: Raw, minimally processed
    â”‚   â”œâ”€ silver/: Cleaned, validated
    â”‚   â””â”€ gold/: Business-ready, aggregated
    â”‚
    â”œâ”€ patterns/: Pattern memory storage
    â”‚   â””â”€ *.pkl (served to workers on-demand)
    â”‚
    â”œâ”€ archive/: Historical data, backups
    â”‚
    â””â”€ raw/: Unprocessed ingestion data
```

---

## Network Performance

### **Bandwidth Analysis (1GbE = ~125 MB/s)**

| Operation | Size | Time | Frequency |
|-----------|------|------|-----------|
| **Daily data prep** | 10GB (2 workers) | ~80s | Once/day |
| **Pattern fetch** | 100MB | ~0.8s | Per job (optional) |
| **Result transfer** | 4GB max (all SLMs) | ~32s | Per job |
| **Typical job result** | 2GB | ~16s | Per job |

**Daily network usage example (20 jobs/day):**
- Prep: 10GB
- Results: 20 Ã— 2GB = 40GB
- Total: 50GB/day
- Active transfer time: ~10 minutes/day
- **Network utilization: <0.5%** âœ…

---

## Why This Architecture?

### **Advantages Over Single-Machine Design**

#### **1. Data Locality for Processing**
```
Before: Data processing competed with SLM inference for CPU
After:  Data processing runs locally with data lake (zero remote I/O)
```

#### **2. Dedicated Compute for SLMs**
```
Before: 4 VMs sharing 36 cores (complex allocation)
After:  2 VMs, each owns entire NUMA socket (clean isolation)
```

#### **3. Network Efficiency**
```
Before: Cross-VM data transfers (internal bottleneck)
After:  Only start-of-day and end-of-job transfers (batched)
```

#### **4. Simplified Worker Topology**
```
Before: Mixed workloads (SLM + data + tasks)
After:  Workers = pure compute (homogeneous)
```

#### **5. Better Resource Utilization**
```
NUMA machine: Optimized for CPU-intensive workloads (SLM inference)
UMA machine:  Optimized for I/O-intensive workloads (data processing)
```

---

## Performance Characteristics

### **Inference Phase (Hot Path)**
- **Latency**: Sub-millisecond (local SSD reads/writes)
- **Network**: Zero (all data local to worker)
- **Bottleneck**: CPU (inference computation)

### **Transfer Phase (Cold Path)**
- **Throughput**: 125 MB/s (1GbE)
- **Typical time**: 16 seconds (2GB)
- **Worst case**: 32 seconds (4GB)
- **Overhead**: <1% of total job time âœ…

### **Job Timeline (Example)**
```
00:00 - Job starts
00:00 - Fetch pattern (optional): 0.8s
00:00 - Load input from SSD: 0.1s
00:01 - Inference: 5 minutes
05:01 - Write output to SSD: 0.5s
05:02 - Signal Temporal
05:02 - Transfer results: 16s (2GB typical)
05:18 - Job complete

Total: 5m 18s
Inference: 5m 0s
Overhead: 18s (6% of total) âœ…
```

---

## Failure Modes & Recovery

### **Worker Node Failure**
```
Impact: 
- Jobs on failed worker are lost
- Other worker continues operating
- Data server unaffected

Recovery:
1. SSH to hypervisor
2. Restart failed VM
3. Temporal retries failed jobs
4. Resume operations
```

### **Data Server Failure**
```
Impact:
- K3s control plane unavailable (no new scheduling)
- Temporal workflows paused
- Workers can't transfer results (staging fills up)
- Pattern fetches fail (if not cached)

Recovery:
1. SSH to data server
2. Restart services (K3s, Temporal)
3. Workers retry transfers automatically
4. Resume operations

Acceptable for home lab use (manual recovery OK)
```

### **Network Failure**
```
Impact:
- Workers can't transfer results
- Staging disk accumulates data
- Pattern fetches fail (if not cached)

Recovery:
- Workers continue inference (data local)
- Results queue in staging (100GB buffer)
- Transfers resume when network restored
```

---

## Monitoring & Observability

### **Key Metrics to Monitor**

**Workers:**
```bash
# Staging disk usage
df -h /data/local
# Alert if > 70%

# CPU usage
top -H
# Should show pinned cores at ~100% during inference

# Network throughput
iftop -i eth0
# Should spike during transfers, idle during inference
```

**Data Server:**
```bash
# Lake disk usage
df -h /data/lake
# Alert if > 90%

# Transfer queue
curl http://localhost:8080/transfer-status
# Monitor pending transfers

# Temporal workflows
tctl workflow list
# Check for stuck workflows
```

---

## Future Expansion

### **Adding More Worker Machines**

Easy to scale horizontally:

```
1. Provision new NUMA machine
2. Create 2 worker VMs (same config)
3. Join to K3s cluster
4. Daily prep distributes data to all workers
5. Temporal schedules jobs across all workers
```

**Benefits:**
- Linear scaling (2 machines = 2Ã— capacity)
- No architectural changes needed
- Same data flow pattern

### **Upgrading Network to 10GbE**

If needed in the future:

```
Current (1GbE):
- 4GB transfer: 32 seconds
- Typical 2GB: 16 seconds

With 10GbE:
- 4GB transfer: 3.2 seconds
- Typical 2GB: 1.6 seconds

Benefit: Negligible for current workload (<1% improvement)
Recommendation: Not worth the cost
```

---

## References

- Original single-machine design: `docs/architecture/numa-design.md`
- VM specifications: `docs/architecture/vm-specifications.md`
- Storage design: `docs/architecture/storage-design.md`
- Implementation guide: `docs/guides/implementation.md`

---

## Summary

**Architecture Type:** Distributed two-machine cluster

**Key Characteristics:**
- âœ… Clean separation: compute vs. storage
- âœ… Data locality: processing local to lake
- âœ… Network efficiency: batched transfers
- âœ… Simple topology: 2 homogeneous workers
- âœ… Appropriate for 1GbE network
- âœ… Manual recovery acceptable (home lab)

**Perfect for:**
- CPU-intensive SLM workloads
- Moderate data transfer volumes (< 100GB/day)
- Home lab environments
- Development and experimentation

**Not suitable for:**
- Production systems requiring HA
- Ultra-low latency requirements (< 1s end-to-end)
- Massive data volumes (> 1TB/day transfers)

