---
# Example SLM Inference Pod
# This demonstrates proper resource allocation and volume mounting
# for SLM pods co-located with Temporal workers

apiVersion: v1
kind: Pod
metadata:
  name: slm-gemma-7b
  namespace: slm
  labels:
    app: slm-inference
    model: gemma-7b
    size: large
spec:
  # Schedule on SLM worker nodes
  nodeSelector:
    workload: slm
  
  # Spread across NUMA nodes for better resource utilization
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - slm-inference
          topologyKey: numa-node
  
  containers:
  - name: inference
    image: your-registry/slm-gemma-7b:latest  # Replace with your SLM image
    imagePullPolicy: IfNotPresent
    
    env:
    - name: MODEL_NAME
      value: "gemma-7b"
    - name: MODEL_PATH
      value: "/models/gemma-7b"
    - name: QUANTIZATION
      value: "INT8"  # INT8 or INT4
    - name: MAX_CONTEXT_LENGTH
      value: "25000"  # 25k tokens
    - name: BATCH_SIZE
      value: "1"
    - name: NUM_THREADS
      value: "8"  # Will use up to 10 cores (burstable)
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    
    # Resource allocation: Burstable QoS
    # Can burst to 10 cores when Temporal worker is idle
    resources:
      requests:
        memory: "20Gi"      # Guaranteed 20GB
        cpu: "6000m"        # Guaranteed 6 cores
      limits:
        memory: "30Gi"      # Can use up to 30GB
        cpu: "10000m"       # Can burst to 10 cores
    
    volumeMounts:
    # Local staging directory (shared with Temporal worker)
    - name: local-staging
      mountPath: /data/local
    
    # Model weights (optional: can be baked into image or downloaded)
    - name: model-cache
      mountPath: /models
      readOnly: true
    
    livenessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 60
      periodSeconds: 30
    
    readinessProbe:
      httpGet:
        path: /ready
        port: 8000
      initialDelaySeconds: 30
      periodSeconds: 10
    
    ports:
    - containerPort: 8000
      name: http
      protocol: TCP
  
  volumes:
  # Local staging volume (hostPath on worker VM)
  # CRITICAL: Same mount as Temporal worker for zero-copy data sharing
  - name: local-staging
    hostPath:
      path: /data/local
      type: Directory
  
  # Model cache (optional: PV for shared model weights)
  - name: model-cache
    hostPath:
      path: /var/lib/models  # Or use PV/PVC
      type: DirectoryOrCreate

---
# Example: Smaller model (different resource profile)
apiVersion: v1
kind: Pod
metadata:
  name: slm-gemma-2b
  namespace: slm
  labels:
    app: slm-inference
    model: gemma-2b
    size: small
spec:
  nodeSelector:
    workload: slm
  
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - slm-inference
          topologyKey: numa-node
  
  containers:
  - name: inference
    image: your-registry/slm-gemma-2b:latest
    
    env:
    - name: MODEL_NAME
      value: "gemma-2b"
    - name: MODEL_PATH
      value: "/models/gemma-2b"
    - name: QUANTIZATION
      value: "INT8"
    - name: MAX_CONTEXT_LENGTH
      value: "25000"
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    
    # Smaller model = fewer resources
    resources:
      requests:
        memory: "10Gi"      # Guaranteed 10GB
        cpu: "4000m"        # Guaranteed 4 cores
      limits:
        memory: "15Gi"      # Can use up to 15GB
        cpu: "8000m"        # Can burst to 8 cores
    
    volumeMounts:
    - name: local-staging
      mountPath: /data/local
    - name: model-cache
      mountPath: /models
      readOnly: true
    
    livenessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 60
      periodSeconds: 30
    
    readinessProbe:
      httpGet:
        path: /ready
        port: 8000
      initialDelaySeconds: 30
      periodSeconds: 10
    
    ports:
    - containerPort: 8000
      name: http
  
  volumes:
  - name: local-staging
    hostPath:
      path: /data/local
      type: Directory
  - name: model-cache
    hostPath:
      path: /var/lib/models
      type: DirectoryOrCreate

---
# Resource Summary (Per VM with 34 cores, 120GB RAM):
#
# Temporal Worker:
#   requests: 2 cores, 4GB (guaranteed)
#   limits: 2 cores, 8GB
#
# 3× SLM Pods (example):
#   Gemma-7B: 6-10 cores, 20-30GB
#   Mistral-7B: 6-10 cores, 20-30GB
#   Gemma-2B: 4-8 cores, 10-15GB
#
# Total Guaranteed: 2 + 6 + 6 + 4 = 18 cores, 54GB RAM
# Total Burst: 2 + 10 + 10 + 8 = 30 cores, 83GB RAM
# Headroom: 4 cores, 37GB RAM ✅
#
# How K8s schedules:
# 1. Reserves 18 cores minimum (won't schedule if not available)
# 2. Allows bursting to 30 cores when idle CPU available
# 3. Temporal worker ALWAYS gets 2 cores (even during SLM burst)
# 4. SLM pods throttled first during CPU pressure

